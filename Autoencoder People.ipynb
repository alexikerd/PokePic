{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "import random\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from skimage import util\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import face_recognition as fr\n",
    "\n",
    "\n",
    "# https://hackernoon.com/how-to-autoencode-your-pok%C3%A9mon-6b0f5c7b7d97\n",
    "# https://www.cs.toronto.edu/~lczhang/360/lec/w05/autoencoder.html\n",
    "# https://www.youtube.com/watch?v=IKOHHItzukk&list=PLZbbT5o_s2xrfNyHZsM6ufI0iZENK9xgG&index=18\n",
    "# https://discuss.pytorch.org/t/layer-reshape-issue/18938/4\n",
    "# https://towardsdatascience.com/find-similar-images-using-autoencoders-315f374029ea\n",
    "# https://towardsdatascience.com/build-a-simple-image-retrieval-system-with-an-autoencoder-673a262b7921\n",
    "\n",
    "# I need to test how multiple faces works with face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = '/People Pictures/'\n",
    "IMG_SIZE = 50\n",
    "CURRENT_DIR = path.abspath(path.curdir)\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 25\n",
    "LR = 0.001\n",
    "\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "#     transforms.ColorJitter(brightness=0.8, contrast=0, saturation=0, hue=0),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13898/13898 [08:55<00:00, 25.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(CURRENT_DIR + FOLDER)\n",
    "images = []\n",
    "\n",
    "\n",
    "for file in tqdm(files):\n",
    "    \n",
    "    image = cv2.imread(os.path.join(CURRENT_DIR + FOLDER,file),0)\n",
    "    face_locations = fr.face_locations(image)\n",
    "\n",
    "    try:\n",
    "        image = image[face_locations[0][0]:face_locations[0][2], face_locations[0][3]:face_locations[0][1]]\n",
    "        image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
    "        images.append(image)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(len(images))\n",
    "\n",
    "data_set = pd.DataFrame({'image':images}).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_set = data_set.sample(frac=0.75, random_state=0)\n",
    "test_set = data_set.drop(train_set.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increase_brightness(img, value):\n",
    "\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "\n",
    "    if value>0:\n",
    "        lim = 255 - value\n",
    "        v[v > lim] = 255\n",
    "        v[v <= lim] += value\n",
    "    elif value<0:\n",
    "        lim = -1*value\n",
    "        v[v<lim] = 0\n",
    "        v[v>=lim] -= lim\n",
    "\n",
    "    final_hsv = cv2.merge((h, s, v))\n",
    "    img = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[0],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50, 50)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# noise = np.random.normal(0, 10, images[0].shape)\n",
    "image_with_noise = util.random_noise(images[0],var=0.001)\n",
    "image_with_noise.shape\n",
    "# image_with_noise = increase_brightness(image_with_noise, np.random.randint(-50,51))\n",
    "# image_with_noise = torch.flip(torch.tensor(image_with_noise), dims=(1,)) \n",
    "# plt.imshow(image_with_noise,cmap='gray')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self,data,transform=img_transform):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "\n",
    "        item = self.data.iloc[index]\n",
    "        image = item[0]/255\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(image)\n",
    "        \n",
    "        return image.reshape(1,IMG_SIZE,IMG_SIZE)\n",
    "    \n",
    "dataset = ImageDataset(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 50, 50])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(dataloader))\n",
    "images = sample\n",
    "images.shape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.Conv1 = nn.Conv2d(1,16,3)\n",
    "        self.Conv2 = nn.Conv2d(16,32,3)\n",
    "        self.Conv3 = nn.Conv2d(32,64,7)\n",
    "\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2,2,return_indices=True)\n",
    "        \n",
    "\n",
    "        self.ConvTrans3 = nn.ConvTranspose2d(64,32,7)\n",
    "        self.ConvTrans2 = nn.ConvTranspose2d(32,16,3)\n",
    "        self.ConvTrans1 = nn.ConvTranspose2d(16,1,3)\n",
    "        \n",
    "        self.unpool = nn.MaxUnpool2d(2,2)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.Conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x, indices1 = self.pool(x)\n",
    "        x = self.Conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x, indices2 = self.pool(x)\n",
    "        x = self.Conv3(x)\n",
    "        \n",
    "        x = self.ConvTrans3(x)\n",
    "        x = self.unpool(x,indices2)\n",
    "        x = F.relu(x)\n",
    "        x = self.ConvTrans2(x)\n",
    "        x = self.unpool(x,indices1)\n",
    "        x = F.relu(x)\n",
    "        x = self.ConvTrans1(x)\n",
    "        x = self.sig(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def encode(self,x):\n",
    "        \n",
    "        x = self.Conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x, indices1 = self.pool(x)\n",
    "        x = self.Conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x, indices2 = self.pool(x)\n",
    "        x = self.Conv3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder().float()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR,\n",
    "                             weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 50, 50])\n",
      "torch.Size([32, 1, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(dataloader))\n",
    "images = sample\n",
    "images = images.float()\n",
    "print(images.shape)\n",
    "with torch.no_grad():\n",
    "    x = model(images)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model, num_epochs, criterion, optimizer, dataloader):\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    outputs = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for data in dataloader:\n",
    " \n",
    "            img = data\n",
    "            if np.random.randint(0,2)==0:\n",
    "                img = torch.flip(img, dims=(1,))              \n",
    "            noisy_img = util.random_noise(img,var=0.001)\n",
    "            for img in noisy_img:\n",
    "#                 img = img * 255\n",
    "                print(img.squeeze().shape)\n",
    "                img = img.squeeze()\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "#                 img = increase_brightness(img,np.random.randint(-50,51))\n",
    "#             noisy_img = increase_brightness(noisy_img, np.random.randint(-50,51))\n",
    "            \n",
    "            \n",
    "            recon = model(torch.tensor(noisy_img).float())\n",
    "            loss = criterion(recon.double(), img.double())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            break\n",
    "        break\n",
    "\n",
    "        print(f'Epoch:{epoch+1}, Loss:{float(loss)}')\n",
    "        outputs.append((epoch, img, recon),)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50)\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.2.0) d:\\bld\\libopencv_1578930274633\\work\\modules\\imgproc\\src\\color.simd_helpers.hpp:94: error: (-2:Unspecified error) in function '__cdecl cv::impl::`anonymous-namespace'::CvtHelper<struct cv::impl::`anonymous namespace'::Set<1,-1,-1>,struct cv::impl::A0x8568db86::Set<3,4,-1>,struct cv::impl::A0x8568db86::Set<0,2,5>,2>::CvtHelper(const class cv::_InputArray &,const class cv::_OutputArray &,int)'\n> Unsupported depth of input image:\n>     'VDepth::contains(depth)'\n> where\n>     'depth' is 6 (CV_64F)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-b1d9a4b5bc87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-79-794ac83ee2db>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, num_epochs, criterion, optimizer, dataloader)\u001b[0m\n\u001b[0;32m     14\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                 \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_GRAY2BGR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;31m#                 img = increase_brightness(img,np.random.randint(-50,51))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m#             noisy_img = increase_brightness(noisy_img, np.random.randint(-50,51))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.2.0) d:\\bld\\libopencv_1578930274633\\work\\modules\\imgproc\\src\\color.simd_helpers.hpp:94: error: (-2:Unspecified error) in function '__cdecl cv::impl::`anonymous-namespace'::CvtHelper<struct cv::impl::`anonymous namespace'::Set<1,-1,-1>,struct cv::impl::A0x8568db86::Set<3,4,-1>,struct cv::impl::A0x8568db86::Set<0,2,5>,2>::CvtHelper(const class cv::_InputArray &,const class cv::_OutputArray &,int)'\n> Unsupported depth of input image:\n>     'VDepth::contains(depth)'\n> where\n>     'depth' is 6 (CV_64F)\n"
     ]
    }
   ],
   "source": [
    "test = train(model, NUM_EPOCHS, criterion, optimizer, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),CURRENT_DIR + 'FaceModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (Conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (Conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (Conv3): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (ConvTrans3): ConvTranspose2d(64, 32, kernel_size=(7, 7), stride=(1, 1))\n",
       "  (ConvTrans2): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (ConvTrans1): ConvTranspose2d(16, 1, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (unpool): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Autoencoder().float()\n",
    "model.load_state_dict(torch.load(CURRENT_DIR + 'FaceModel'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = ImageDataset(test_set)\n",
    "testloader = dataloader = DataLoader(\n",
    "    testset, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = next(iter(testloader))\n",
    "images = sample\n",
    "grid = torchvision.utils.make_grid(images,nrow=8)\n",
    "plt.figure(figsize=(50,50))\n",
    "plt.imshow(grid.numpy().transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    test_images = model(images.float())\n",
    "test_grid = torchvision.utils.make_grid(test_images,nrow=8)\n",
    "plt.figure(figsize=(50,50))\n",
    "plt.imshow(test_grid.numpy().transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalset = ImageDataset(data_set)\n",
    "finalloader = dataloader = DataLoader(\n",
    "    finalset, \n",
    "    batch_size=len(data_set),\n",
    "    shuffle=True\n",
    "    )\n",
    "og_images = next(iter(dataloader))\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    encodings = model.encode(og_images.float())\n",
    "encodings = encodings.reshape((-1, np.prod(1600)))\n",
    "encodings = encodings.numpy()\n",
    "# og_images = og_images.numpy().transpose(3,1,2,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10780, 1600)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='auto', leaf_size=30, metric='cosine',\n",
       "                 metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                 radius=1.0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = NearestNeighbors(n_neighbors=5, metric=\"cosine\")\n",
    "knn.fit(np.asarray(encodings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = knn.kneighbors(encodings[16].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(og_images[16].permute(1,2,0).squeeze(),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_images = og_images[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_grid = torchvision.utils.make_grid(closest_images,nrow=5)\n",
    "plt.figure(figsize=(50,50))\n",
    "plt.imshow(closest_grid.numpy().transpose(1,2,0).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "CELEB_FOLDER = '/Celeb Pictures/'\n",
    "celeb_files = os.listdir(CURRENT_DIR + CELEB_FOLDER)\n",
    "celeb_images = []\n",
    "celeb_pokemons = []\n",
    "\n",
    "for file in tqdm(celeb_files):\n",
    "    \n",
    "    celeb_image = cv2.imread(os.path.join(CURRENT_DIR + CELEB_FOLDER,file),0)\n",
    "    face_locations = fr.face_locations(celeb_image)\n",
    "    celeb_pokemons.append(file.split('.')[0])\n",
    "\n",
    "    \n",
    "    celeb_image = celeb_image[face_locations[0][0]:face_locations[0][2], face_locations[0][3]:face_locations[0][1]]\n",
    "    celeb_image = cv2.resize(celeb_image,(IMG_SIZE,IMG_SIZE))\n",
    "    celeb_images.append(celeb_image)\n",
    "\n",
    "    \n",
    "celebset = ImageDataset(pd.DataFrame({'image':celeb_images}).sample(frac=1).reset_index(drop=True))\n",
    "celebloader = DataLoader(\n",
    "    celebset, \n",
    "    batch_size=len(celeb_images),\n",
    "    shuffle=True\n",
    "    )\n",
    "c_images = next(iter(celebloader))\n",
    "plt.imshow(celeb_images[3],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image = celeb_images[3]\n",
    "print(new_image.shape)\n",
    "print(new_image.dtype)\n",
    "new_image = increase_brightness(new_image,np.random.randint(-100,101))\n",
    "plt.imshow(new_image,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dugtrio', 'Jinx', 'Rapidash', 'Regirock']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celeb_pokemons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1600)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    encodings = model.encode(c_images.float())\n",
    "encodings = encodings.reshape((-1, np.prod(1600)))\n",
    "encodings = encodings.numpy()\n",
    "encodings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10780, 1, 50, 50])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = knn.kneighbors(encodings[2].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(c_images[3].permute(1,2,0).squeeze(),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = knn.kneighbors(encodings[0].reshape(1,-1))\n",
    "closest_images = og_images[indices]\n",
    "closest_grid = torchvision.utils.make_grid(closest_images,nrow=5)\n",
    "plt.figure(figsize=(50,50))\n",
    "plt.imshow(closest_grid.numpy().transpose(1,2,0).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.27843137 0.41176471 0.30980392 ... 0.17254902 0.43921569 0.4       ]\n",
      " [0.31764706 0.27843137 0.25882353 ... 0.25490196 0.3372549  0.30588235]\n",
      " [0.18431373 0.28235294 0.30980392 ... 0.18431373 0.31764706 0.34117647]\n",
      " ...\n",
      " [0.82352941 0.79607843 0.6627451  ... 0.14901961 0.23529412 0.15294118]\n",
      " [0.74901961 0.53333333 0.17647059 ... 0.19215686 0.21960784 0.19215686]\n",
      " [0.38823529 0.14901961 0.12941176 ... 0.24313725 0.19607843 0.14509804]]\n"
     ]
    }
   ],
   "source": [
    "for data in dataloader:\n",
    "    imgs = data\n",
    "    \n",
    "    noisy_imgs = util.random_noise(imgs,var=0.001)\n",
    "    \n",
    "\n",
    "    noisy_imgs = noisy_imgs.squeeze()\n",
    "    noisy_imgs = noisy_imgs * 255\n",
    "    noisy_imgs = noisy_imgs.astype('uint8')\n",
    "    for i in range(len(noisy_imgs)):\n",
    "        noisy_imgs[i] = increase_brightness(noisy_imgs[i],np.random.randint(-50,50))\n",
    "\n",
    "    noisy_imgs = noisy_imgs.astype('float')\n",
    "    noisy_imgs = noisy_imgs/255\n",
    "    print(noisy_imgs[0])\n",
    "\n",
    "    \n",
    "    \n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
